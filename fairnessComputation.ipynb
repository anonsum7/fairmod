{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fairness_metrics\n",
    "import utils\n",
    "import ast\n",
    "import pickle\n",
    "import operator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_score(row):\n",
    "    row = ast.literal_eval(row)\n",
    "    # max_score = sum(list(row.values()))\n",
    "    max_score = max(row.items(), key=operator.itemgetter(1))[1]\n",
    "    return max_score\n",
    "\n",
    "def get_max_class(row):\n",
    "    row = regard_map[row]\n",
    "    # row = ast.literal_eval(row)\n",
    "    max_class = max(row.items(), key=operator.itemgetter(1))[0]\n",
    "    return max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender based subset\n",
    "gender_features = ['male', 'female', 'transgender', 'other_gender']\n",
    "# race based subset\n",
    "race_features = ['asian', 'black', 'latino', 'white', 'other_race_or_ethnicity']\n",
    "# disability based subset\n",
    "disability_features = ['intellectual_or_learning_disability', 'physical_disability', 'psychiatric_or_mental_illness', 'other_disability']\n",
    "# sexual orientation subset\n",
    "sexual_orientation_features = ['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']\n",
    "\n",
    "labels = [gender_features, race_features, disability_features, sexual_orientation_features]\n",
    "names = ['gender', 'race', 'disability', 'sexual_orientation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the relevant datasets\n",
    "ideology_df = pd.read_csv('./data/processed_annotated_comments.csv')\n",
    "ideology_df['label'] = ideology_df['label'].apply(lambda x: None if x not in ['left', 'right'] else x)\n",
    "ideology_df.dropna(inplace=True)\n",
    "\n",
    "gender_df = pd.read_csv('./data/jigsaw/gender.csv')\n",
    "race_df = pd.read_csv('./data/jigsaw/race.csv')\n",
    "disability_df = pd.read_csv('./data/jigsaw/disability.csv')\n",
    "sexual_orientation_df = pd.read_csv('./data/jigsaw/sexual_orientation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_moderated(df, moderation_map):\n",
    "    moderations = []\n",
    "    new_df = df.copy()\n",
    "    for text in df['text']:\n",
    "        if text in moderation_map.keys():\n",
    "            moderations += [moderation_map[text]]\n",
    "        else:\n",
    "            new_df = new_df[new_df.text != text]\n",
    "\n",
    "    df = new_df.copy()\n",
    "\n",
    "    df['flagged'] = moderations\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ideology_df, gender_df, race_df, disability_df, sexual_orientation_df]\n",
    "labels = [list(set(df['label'])) for df in datasets]\n",
    "names = ['ideology', 'gender', 'race', 'disability', 'sexual orientation']\n",
    "factors = ['positive', 'negative', 'neutral', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases to consider\n",
    "label_map = {}\n",
    "label_map['ideology'] = 'left' \n",
    "label_map['gender'] = 'male'\n",
    "label_map['race'] = 'white'\n",
    "label_map['disability'] = 'physical_disability'\n",
    "label_map['sexual orientation'] = 'heterosexual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(datasets, names, moderation_map):\n",
    "    metrics = []\n",
    "    for dataset, name in zip(datasets, names):\n",
    "        dataset = make_moderated(dataset.copy(), moderation_map)\n",
    "        subset = utils.process_dataset(dataset.copy(), label_map[name])\n",
    "        # demographic parity\n",
    "        dp = fairness_metrics.demographic_parity(df=subset.copy())\n",
    "        \n",
    "        # add regard\n",
    "        regard = subset.copy()['text'].apply(get_max_class)\n",
    "\n",
    "        csp = []\n",
    "        for factor in factors:\n",
    "            csp += [fairness_metrics.conditional_statistical_parity(df=subset.copy(), factors=regard, factor=factor)]\n",
    "        csp += [dp, name, label_map[name]]\n",
    "        metrics += [csp]\n",
    "    columns = factors+['dp', 'bias', 'group']\n",
    "    columns = [column.upper() for column in columns]\n",
    "    df = pd.DataFrame(metrics, columns = columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you have saved the regard model results\n",
    "\n",
    "# with open('./results/<regard_map_location>.pkl', 'rb') as handle: 3 comment_regard\n",
    "    # regard_map = pickle.load(handle)\n",
    "\n",
    "# uncomment if you have saved the moderation API results\n",
    "\n",
    "# with open('./results/<openai_moderation_path>.pkl', 'rb') as handle: # comments_moderated_openai\n",
    "    # moderation_map_openai = pickle.load(handle)\n",
    "\n",
    "# with open('./results/<uniformly_random_baseline>.pkl', 'rb') as handle: # unirand_comments\n",
    "    # moderation_map_unirand = pickle.load(handle)\n",
    "\n",
    "# with open('./results/<clarifai_moderation_path>.pkl', 'rb') as handle: # comment_moderation_clarifai\n",
    "    # moderation_map_clarifai = pickle.load(handle)\n",
    "\n",
    "# with open('./results/<google_perspective_moderation_path>.pkl', 'rb') as handle: # comments_moderation_google_perspective\n",
    "    # moderation_map_perspective = pickle.load(handle)\n",
    "\n",
    "# with open('./results/<google_palm_moderation_path>.pkl', 'rb') as handle: # comments_moderation_google\n",
    "    # moderation_map_google = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if processing raw moderation results\n",
    "# moderation_map_openai = utils.process_openai(moderation_map_openai)\n",
    "# moderation_map_clarifai = utils.process_clarifai(moderation_map_clarifai)\n",
    "# moderation_map_perspective = utils.process_perspective(moderation_map_perspective)\n",
    "# moderation_map_google = utils.process_google(moderation_map_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed results\n",
    "with open('./results/moderation_results.pkl', 'rb') as file:\n",
    "    fairness_results = pickle.load(file)\n",
    "\n",
    "moderation_map_unirand = {k:v['unirand'] for k,v in fairness_results.items()}\n",
    "moderation_map_openai = {k:v['openai'] for k,v in fairness_results.items()}\n",
    "moderation_map_clarifai = {k:v['clarifai'] for k,v in fairness_results.items()}\n",
    "moderation_map_perspective = {k:v['perspective'] for k,v in fairness_results.items()}\n",
    "moderation_map_google = {k:v['google'] for k,v in fairness_results.items() if v['google'] in [True, False]}\n",
    "regard_map = {k:v['regard'] for k,v in fairness_results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unirand = get_metrics(datasets=datasets.copy(), names=names, moderation_map=moderation_map_unirand)\n",
    "df_openai = get_metrics(datasets=datasets.copy(), names=names, moderation_map=moderation_map_openai)\n",
    "df_clarifai = get_metrics(datasets=datasets.copy(), names=names, moderation_map=moderation_map_clarifai)\n",
    "df_google = get_metrics(datasets=datasets.copy(), names=names, moderation_map=moderation_map_google)\n",
    "df_perspective = get_metrics(datasets=datasets.copy(), names=names, moderation_map=moderation_map_perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unirand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clarifai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "import fairness_visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"ticks\")\n",
    "color_1 = (sns.color_palette(\"pastel\")[5], sns.color_palette(\"pastel\")[5], sns.color_palette(\"pastel\")[5])\n",
    "color_2 = (sns.color_palette(\"pastel\")[0], sns.color_palette(\"pastel\")[0], sns.color_palette(\"pastel\")[0])\n",
    "color_3 = (sns.color_palette(\"pastel\")[-3], sns.color_palette(\"pastel\")[-3], sns.color_palette(\"pastel\")[-3])\n",
    "color_4 = (sns.color_palette(\"pastel\")[1], sns.color_palette(\"pastel\")[1], sns.color_palette(\"pastel\")[1])\n",
    "color_5 = (sns.color_palette(\"pastel\")[-1], sns.color_palette(\"pastel\")[-1], sns.color_palette(\"pastel\")[-1])\n",
    "colors = [color_1, color_2, color_3, color_4, color_5]\n",
    "dfs = [df_openai, df_perspective, df_google, df_clarifai]\n",
    "names = ['A', 'B', 'C', 'D']\n",
    "tuples_main_xy = [(a,b) for a,b in zip(df_unirand['NEGATIVE'], df_unirand['DP'])]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(4, 4))\n",
    "ax = ax.flatten()\n",
    "x_max = -1\n",
    "y_max = -1\n",
    "for i in range(0, len(ax)):\n",
    "    tuples_xy = [(a,b) for a,b in zip(dfs[i]['NEGATIVE'], dfs[i]['DP'])]\n",
    "    local_max_y = max(max(dfs[i]['DP'].tolist()), max(df_unirand['DP'].tolist()))\n",
    "    local_max_x = max(max(dfs[i]['NEGATIVE'].tolist()), max(df_unirand['NEGATIVE'].tolist()))\n",
    "    if  local_max_x > x_max:\n",
    "        x_max = local_max_x\n",
    "    if local_max_y > y_max:\n",
    "        y_max = local_max_y\n",
    "\n",
    "\n",
    "    fairness_visualization.plot_specific(ax[i], tuples_main_xy, tuples_xy, colors)\n",
    "    ax[i].text(x=0.88, y=0.85, s=names[i], fontsize=13, weight=\"bold\", transform=ax[i].transAxes)\n",
    "\n",
    "for i in range(0, len(ax)):\n",
    "    ax[i].set_xlim(0, min(x_max+0.05, 1))\n",
    "    ax[i].set_ylim(0, min(y_max+0.05, 1))\n",
    "\n",
    "markers = ['s', '^']\n",
    "marker_texts = ['Baseline Classifier', 'ASM Classifier']\n",
    "new_patches = [plt.plot([], [], marker=markers[i], ms=10, ls=\"\", mec='k', mfc='w',\n",
    "                label=\"{:s}\".format(marker_texts[i]))[0] for i in range(2)]\n",
    "legend1 = fig.legend(handles=new_patches, bbox_to_anchor=(0.54, 0.15),prop={'size': 11},\n",
    "                        loc='center', ncol=5, handletextpad=1, labelspacing=1, columnspacing=0.5, framealpha=0)\n",
    "\n",
    "ax[1].add_artist(legend1)\n",
    "\n",
    "plt.tight_layout(pad=1)\n",
    "fig.subplots_adjust(bottom=0.35)\n",
    "colors = [color_1[0], color_2[0], color_3[0], color_4[0], color_5[0]]\n",
    "texts = [\"Ideology\", \"Gender\", \"Ethnicity\", \"Disability\", \"Sexual Orientation\"]\n",
    "patches = [mpatches.Patch(color=colors[i], label=texts[i]) for i in range(len(texts))]\n",
    "fig.legend(handles=patches, bbox_to_anchor=(0.54, 0.025),prop={'size': 12},\n",
    "loc='center', ncol=3, handletextpad=2, labelspacing=1, columnspacing=0.5, framealpha=0)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
